# -*- coding: utf-8 -*-
"""New 2 - ABSA-BERT only.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d-qsSpTq3UvT4KVvMda_f9UaUTqG6PhO
"""

# ==========================================================
# ABSA-BERT Only (uses existing sentiment columns)
# Dataset columns expected (any case-insensitive match):
#   user_id, item_id, reviewText, sentiment (score in [-1,1]),
#   sentiment_ (label), sentiment_numeric (-1/0/+1), rating?, timestamp?
# ==========================================================

!pip install -q pandas numpy torch==2.3.1 tqdm openpyxl

import os, re, math, random, numpy as np, pandas as pd
from collections import defaultdict
from tqdm import tqdm
import torch, torch.nn as nn
import torch.nn.functional as F
from google.colab import files

SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ---------------------------
# Load Dataset
# ---------------------------
uploaded = files.upload()
INPUT_FILE = list(uploaded.keys())[0]
print("Using:", INPUT_FILE)

def load_any(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in [".csv", ".txt"]:
        return pd.read_csv(path)
    elif ext in [".xls", ".xlsx"]:
        return pd.read_excel(path)
    else:
        raise ValueError("Upload CSV/XLS/XLSX")

df_raw = load_any(INPUT_FILE)
print("Columns:", list(df_raw.columns)[:15])

# ---------------------------
# Standardize columns (match your screenshot)
# ---------------------------
def pick(df, names):
    low = {c.lower(): c for c in df.columns}
    for n in names:
        if n.lower() in low: return low[n.lower()]
    return None

user_col = pick(df_raw, ["user_id","user","uid"])
item_col = pick(df_raw, ["item_id","item","iid","product_id","movieid"])
text_col = pick(df_raw, ["reviewText","review_text","review","text"])
sent_score_col = pick(df_raw, ["sentiment","sentiment_score"])
sent_label_col = pick(df_raw, ["sentiment_","sentiment_label"])
sent_numeric_col = pick(df_raw, ["sentiment_numeric","polarity"])
rating_col = pick(df_raw, ["rating","score","stars"])
ts_col = pick(df_raw, ["timestamp","time","unixReviewTime","date","datetime"])

if user_col is None or item_col is None:
    raise ValueError("Couldn't find user_id / item_id columns.")

# if no text col, just create placeholder (not used further)
if text_col is None:
    df_raw["reviewText"] = ""
    text_col = "reviewText"

# if no sentiment columns, create neutral ones
if sent_score_col is None:
    print("⚠️ No sentiment score column found; using neutral zeros.")
    df_raw["sentiment"] = 0.0
    sent_score_col = "sentiment"
if sent_label_col is None:
    df_raw["sentiment_"] = "neutral"
    sent_label_col = "sentiment_"
if sent_numeric_col is None:
    df_raw["sentiment_numeric"] = 0
    sent_numeric_col = "sentiment_numeric"

cols = [user_col, item_col, text_col, sent_score_col, sent_label_col, sent_numeric_col] \
       + ([rating_col] if rating_col else []) + ([ts_col] if ts_col else [])
df = df_raw[cols].copy()
df.columns = ["user","item","review","sentiment_score","sentiment_label","sentiment_numeric"] \
             + (["rating"] if rating_col else []) + (["timestamp"] if ts_col else [])

# Implicit positives for training
if "rating" in df.columns:
    df["y"] = (pd.to_numeric(df["rating"], errors="coerce").fillna(0) > 0).astype(int)
else:
    df["y"] = 1
df = df[df["y"] > 0].copy()

# Map ids to contiguous ints
df["user"] = df["user"].astype(str)
df["item"] = df["item"].astype(str)
user2id = {u:i for i,u in enumerate(df["user"].astype("category").cat.categories)}
item2id = {v:i for i,v in enumerate(df["item"].astype("category").cat.categories)}
df["u"] = df["user"].map(user2id)
df["i"] = df["item"].map(item2id)
n_users, n_items = len(user2id), len(item2id)
print(f"#Users={n_users}, #Items={n_items}, #Interactions={len(df)}")

# ---------------------------
# Leave-One-Out (time-aware if timestamp)
# ---------------------------
def leave_one_out_split(df_ui):
    frames=[]
    if "timestamp" in df_ui.columns:
        for _,g in df_ui.groupby("u"):
            frames.append(g.sort_values("timestamp"))
    else:
        for _,g in df_ui.groupby("u"):
            frames.append(g.sample(frac=1.0, random_state=SEED))
    train,val,test=[],[],[]
    for g in frames:
        if len(g)==1: test.append(g.iloc[-1])
        elif len(g)==2: val.append(g.iloc[-2]); test.append(g.iloc[-1])
        else:
            train.extend(list(g.iloc[:-2].itertuples(index=False)))
            val.append(g.iloc[-2]); test.append(g.iloc[-1])
    cols=df_ui.columns
    train=pd.DataFrame(train, columns=cols) if train else df_ui.head(0)
    val  =pd.DataFrame(val,   columns=cols)
    test =pd.DataFrame(test,  columns=cols)
    return train.reset_index(drop=True), val.reset_index(drop=True), test.reset_index(drop=True)

base_cols = ["u","i","review","sentiment_score","sentiment_label","sentiment_numeric"] \
            + (["rating"] if "rating" in df.columns else []) \
            + (["timestamp"] if "timestamp" in df.columns else [])
train_df, val_df, test_df = leave_one_out_split(df[base_cols])
print("#Train:", len(train_df), "#Val:", len(val_df), "#Test:", len(test_df))

# lookups
def u2pos(df_):
    d=defaultdict(set)
    for u,i in zip(df_["u"].values, df_["i"].values): d[u].add(i)
    return d
u_train_pos=u2pos(train_df); u_val_pos=u2pos(val_df); u_test_pos=u2pos(test_df)

def make_senti_lookup(df_part):
    d={}
    for r in df_part.itertuples(index=False):
        d[(int(r.u), int(r.i))] = float(r.sentiment_score)
    return d
senti_train = make_senti_lookup(train_df)
senti_val   = make_senti_lookup(val_df)
senti_test  = make_senti_lookup(test_df)

# ---------------------------
# Sentiment-aware BPR-MF
#   score(u,i) = <P_u,Q_i> + w_s * sentiment_score[u,i]
# ---------------------------
class BPRMF_Sent(nn.Module):
    def __init__(self, n_users, n_items, dim=64):
        super().__init__()
        self.U = nn.Embedding(n_users, dim)
        self.I = nn.Embedding(n_items, dim)
        nn.init.xavier_uniform_(self.U.weight); nn.init.xavier_uniform_(self.I.weight)
        self.w_s = nn.Parameter(torch.tensor(0.5))

    def score(self, users, items, senti_dict):
        dot = (self.U(users) * self.I(items)).sum(dim=1)
        pairs = [(int(u.item()), int(i.item())) for u,i in zip(users, items)]
        s = torch.tensor([senti_dict.get(p, 0.0) for p in pairs], dtype=torch.float32, device=users.device)
        return dot + self.w_s * s

def sample_bpr_batch(u_pos_dict, n_items, batch_size=4096):
    keys=[u for u,v in u_pos_dict.items() if len(v)>0]
    users,pos,neg=[],[],[]
    for _ in range(batch_size):
        u=random.choice(keys)
        ip=random.choice(tuple(u_pos_dict[u]))
        while True:
            j=random.randint(0,n_items-1)
            if j not in u_pos_dict[u]: break
        users.append(u); pos.append(ip); neg.append(j)
    return (torch.tensor(users,device=device),
            torch.tensor(pos,device=device),
            torch.tensor(neg,device=device))

def bpr_loss(pos_scores, neg_scores):
    return -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-8))

model = BPRMF_Sent(n_users, n_items, dim=64).to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)

# ---------------------------
# Evaluation helpers
# ---------------------------
@torch.no_grad()
def get_scores_user_vs_items(model, u, cand_items, senti_dict):
    u_t = torch.tensor([u]*len(cand_items), device=device)
    i_t = torch.tensor(cand_items, device=device)
    s = model.score(u_t, i_t, senti_dict).detach().cpu().numpy()
    return s

@torch.no_grad()
def evaluate(model, train_pos, eval_pos, senti_dict, K_list=(10,20), sample_neg=1000):
    metrics = {f"{m}@{k}":[] for k in K_list for m in ["Recall","NDCG","MRR","Hit"]}
    users = sorted(eval_pos.keys()); all_items = np.arange(n_items)
    for u in users:
        pos_items = list(eval_pos[u]);
        if not pos_items: continue
        seen = train_pos.get(u, set())
        neg_pool = np.setdiff1d(all_items, np.array(list(seen)))
        neg_cands = np.random.choice(neg_pool, size=min(sample_neg, len(neg_pool)), replace=False) if len(neg_pool)>0 else np.array([], int)
        candidates = np.unique(np.concatenate([neg_cands, np.array(pos_items)]))
        scores = get_scores_user_vs_items(model, u, candidates, senti_dict)
        order = np.argsort(-scores); ranked_items = candidates[order]
        for K in K_list:
            topk = ranked_items[:K]
            hits = [1 if it in pos_items else 0 for it in topk]
            recall = sum(hits)/len(pos_items)
            dcg = sum(1.0/math.log2(idx+2) for idx,it in enumerate(topk) if it in pos_items)
            idcg = sum(1.0/math.log2(i+2) for i in range(min(len(pos_items),K)))
            ndcg = dcg/idcg if idcg>0 else 0.0
            rr = 0.0
            for idx,it in enumerate(ranked_items):
                if it in pos_items: rr = 1.0/(idx+1); break
            metrics[f"Recall@{K}"].append(recall)
            metrics[f"NDCG@{K}"].append(ndcg)
            metrics[f"MRR@{K}"].append(rr)
            metrics[f"Hit@{K}"].append(1 if any(hits) else 0)
    return {m: float(np.mean(v)) if v else 0.0 for m,v in metrics.items()}

@torch.no_grad()
def auc_score(model, train_pos, eval_pos, senti_dict, neg_per_pos=50):
    aucs=[]
    for u,pos_set in eval_pos.items():
        pos_items=list(pos_set)
        if not pos_items: continue
        seen=train_pos.get(u,set())
        all_negs=[j for j in range(n_items) if j not in seen]
        if len(all_negs)==0: continue
        negs=random.sample(all_negs,min(neg_per_pos,len(all_negs)))
        sp = get_scores_user_vs_items(model, u, pos_items, senti_dict)
        sn = get_scores_user_vs_items(model, u, negs, senti_dict)
        wins=sum(p>n for p in sp for n in sn)
        total=len(sp)*len(sn)
        if total>0: aucs.append(wins/total)
    return float(np.mean(aucs)) if aucs else 0.0

@torch.no_grad()
def accuracy_score(model, eval_df, senti_dict, threshold=0.5):
    preds=[]
    for r in eval_df.itertuples(index=False):
        s = model.score(torch.tensor([int(r.u)],device=device),
                        torch.tensor([int(r.i)],device=device),
                        senti_dict)[0].item()
        preds.append(s)
    preds=np.array(preds)
    preds_norm=(preds - preds.min())/(preds.max()-preds.min()+1e-8)
    if "rating" in eval_df.columns:
        labels=(pd.to_numeric(eval_df["rating"],errors="coerce").fillna(0).values>0).astype(int)
    else:
        labels=np.ones_like(preds_norm)
    return float(((preds_norm>=threshold).astype(int)==labels).mean())

@torch.no_grad()
def rmse_score(model, eval_df, senti_dict):
    preds=[]
    for r in eval_df.itertuples(index=False):
        s = model.score(torch.tensor([int(r.u)],device=device),
                        torch.tensor([int(r.i)],device=device),
                        senti_dict)[0].item()
        preds.append(s)
    preds=np.array(preds)
    if "rating" in eval_df.columns:
        labels=pd.to_numeric(eval_df["rating"],errors="coerce").fillna(0).values.astype(float)
    else:
        labels=np.ones_like(preds)
    preds_scaled=(preds-preds.min())/(preds.max()-preds.min()+1e-8)*(labels.max()-labels.min())+labels.min()
    return float(np.sqrt(np.mean((preds_scaled-labels)**2)))

# ---------------------------
# Train
# ---------------------------
EPOCHS=40
BATCH_SIZE=4096
eval_every=5
print("Start training ABSA-BERT only (uses provided sentiment scores)...")
for epoch in range(1, EPOCHS+1):
    model.train()
    users = []; pos_items = []; neg_items = []
    keys=[u for u,v in u_train_pos.items() if len(v)>0]
    for _ in range(BATCH_SIZE):
        u=random.choice(keys)
        ip=random.choice(tuple(u_train_pos[u]))
        while True:
            j=random.randint(0,n_items-1)
            if j not in u_train_pos[u]: break
        users.append(u); pos_items.append(ip); neg_items.append(j)
    users=torch.tensor(users,device=device); pos_items=torch.tensor(pos_items,device=device); neg_items=torch.tensor(neg_items,device=device)

    pos_scores = model.score(users, pos_items, senti_train)
    neg_scores = model.score(users, neg_items, senti_train)
    loss = bpr_loss(pos_scores, neg_scores)
    opt.zero_grad(); loss.backward(); opt.step()

    if epoch % eval_every == 0 or epoch == 1:
        model.eval()
        val_metrics = evaluate(model, u_train_pos, u_val_pos, senti_val, K_list=(10,20))
        val_auc  = auc_score(model, u_train_pos, u_val_pos, senti_val)
        val_acc  = accuracy_score(model, val_df, senti_val)
        val_rmse = rmse_score(model, val_df, senti_val)
        print(f"[Epoch {epoch:03d}] Loss={loss.item():.4f} | "
              f"Val R@10={val_metrics['Recall@10']:.4f} N@10={val_metrics['NDCG@10']:.4f} "
              f"MRR@10={val_metrics['MRR@10']:.4f} Hit@10={val_metrics['Hit@10']:.4f} | "
              f"AUC={val_auc:.4f} Acc={val_acc:.4f} RMSE={val_rmse:.4f} | w_s={model.w_s.item():.3f}")

# ---------------------------
# Final Test Evaluation (table)
# ---------------------------
print("\nFinal evaluation on TEST (ABSA-BERT only):")
model.eval()
test_metrics = evaluate(model, u_train_pos, u_test_pos, senti_test, K_list=(10,20))
test_auc  = auc_score(model, u_train_pos, u_test_pos, senti_test)
test_acc  = accuracy_score(model, test_df, senti_test)
test_rmse = rmse_score(model, test_df, senti_test)

results = {
    "Method": "ABSA-BERT Only",
    "Recall@10": test_metrics['Recall@10'],
    "Recall@20": test_metrics['Recall@20'],
    "NDCG@10":   test_metrics['NDCG@10'],
    "NDCG@20":   test_metrics['NDCG@20'],
    "MRR@10":    test_metrics['MRR@10'],
    "MRR@20":    test_metrics['MRR@20'],
    "Accuracy":  test_acc,
    "AUC":       test_auc,
    "RMSE":      test_rmse
}
print(pd.DataFrame([results]).round(4).to_string(index=False))

