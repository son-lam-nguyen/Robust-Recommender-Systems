# -*- coding: utf-8 -*-
"""New 2 - LightGCN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fchcr3OGjcOTcjEBzuCu4bTZR1UCg1gE
"""

# ============================================
# Baseline LightGCN (Colab-ready)
# - Upload CSV/XLS/XLSX
# - Leave-one-out split (time-aware if timestamp available)
# - BPR training
# - Metrics: Recall@K, NDCG@K, MRR, Hit@K, AUC, Accuracy, RMSE
# ============================================

!pip install -q pandas numpy scipy torch==2.3.1 tqdm openpyxl

import os, math, random, numpy as np, pandas as pd
from collections import defaultdict
from tqdm import tqdm
import torch, torch.nn as nn
import torch.nn.functional as F
from scipy.sparse import coo_matrix
from google.colab import files

# -------------------------------
# Repro / device
# -------------------------------
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Device:", device)

# -------------------------------
# Upload & Load Dataset1
# -------------------------------
uploaded = files.upload()
INPUT_FILE = list(uploaded.keys())[0]
print("Using:", INPUT_FILE)

def load_any(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in ['.csv', '.txt']:
        df = pd.read_csv(path)
    elif ext in ['.xls', '.xlsx']:
        df = pd.read_excel(path)
    else:
        raise ValueError("Upload CSV/XLS/XLSX.")
    return df

df_raw = load_any(INPUT_FILE)
print("Columns:", list(df_raw.columns))
print(df_raw.head())

# -------------------------------
# 1) Standardize columns
# -------------------------------
# -------------------------------
# Standardize columns
# -------------------------------
def guess_col(df, groups):
    low = {c.lower(): c for c in df.columns}
    for g in groups:
        for c in g:
            if c.lower() in low:
                return low[c.lower()]
    return None

user_col = guess_col(df_raw, [['user','user_id','uid','userid','User','UserID','UID']])
item_col = guess_col(df_raw, [['item','item_id','iid','movieId','product_id','Item','ItemID','IID']])
rating_col = guess_col(df_raw, [['rating','ratings','score','rate','Score']])
ts_col     = guess_col(df_raw, [['timestamp','time','ts','date','datetime','created_at']])

if user_col is None or item_col is None:
    user_col, item_col = df_raw.columns[:2]
    print("Fallback to first two columns as (user,item):", user_col, item_col)

cols = [user_col, item_col] + ([rating_col] if rating_col else []) + ([ts_col] if ts_col else [])
df = df_raw[cols].copy()
df.rename(columns={user_col:'user', item_col:'item'}, inplace=True)
if rating_col: df.rename(columns={rating_col:'rating'}, inplace=True)
if ts_col:     df.rename(columns={ts_col:'timestamp'}, inplace=True)

# Binarize interactions (implicit)
if 'rating' in df.columns:
    df['y'] = (pd.to_numeric(df['rating'], errors='coerce').fillna(0) > 0).astype(int)
else:
    df['y'] = 1
df = df[df['y'] > 0]  # keep positive interactions for graph building

# Map to contiguous ids
df['user'] = df['user'].astype(str)
df['item'] = df['item'].astype(str)
user2id = {u:i for i,u in enumerate(df['user'].astype('category').cat.categories)}
item2id = {v:i for i,v in enumerate(df['item'].astype('category').cat.categories)}
df['u'] = df['user'].map(user2id)
df['i'] = df['item'].map(item2id)
n_users, n_items = len(user2id), len(item2id)
print(f"#Users={n_users}, #Items={n_items}, #Interactions={len(df)}")

# -------------------------------
# 2) Leave-One-Out split
# -------------------------------
def leave_one_out_split(df_ui):
    # df_ui: columns ['u','i', optional 'timestamp', optional 'rating']
    frames = []
    if 'timestamp' in df_ui.columns:
        for u, g in df_ui.groupby('u'):
            frames.append(g.sort_values('timestamp'))
    else:
        for u, g in df_ui.groupby('u'):
            frames.append(g.sample(frac=1.0, random_state=SEED))

    train, val, test = [], [], []
    for g in frames:
        if len(g)==1:
            test.append(g.iloc[-1])
        elif len(g)==2:
            val.append(g.iloc[-2]); test.append(g.iloc[-1])
        else:
            train.extend(list(g.iloc[:-2].itertuples(index=False)))
            val.append(g.iloc[-2]); test.append(g.iloc[-1])

    cols = df_ui.columns
    train = pd.DataFrame(train, columns=cols) if train else df_ui.head(0)
    val   = pd.DataFrame(val, columns=cols)
    test  = pd.DataFrame(test, columns=cols)
    return train.reset_index(drop=True), val.reset_index(drop=True), test.reset_index(drop=True)

base_cols = ['u','i'] + (['timestamp'] if 'timestamp' in df.columns else []) + (['rating'] if 'rating' in df.columns else [])
train_df, val_df, test_df = leave_one_out_split(df[base_cols])
print("#Train:", len(train_df), "#Val:", len(val_df), "#Test:", len(test_df))

def u2pos(df_):
    d = defaultdict(set)
    for u,i in zip(df_['u'].values, df_['i'].values):
        d[u].add(i)
    return d

u_train_pos = u2pos(train_df)
u_val_pos   = u2pos(val_df)
u_test_pos  = u2pos(test_df)

# -------------------------------
# 3) Normalized adjacency from TRAIN
# -------------------------------
def build_norm_adj(n_users, n_items, train):
    rows, cols, data = [], [], []
    for u,i in zip(train['u'].values, train['i'].values):
        rows.append(u);           cols.append(n_users+i); data.append(1.0)
        rows.append(n_users+i);   cols.append(u);         data.append(1.0)
    N = n_users + n_items
    adj = coo_matrix((data, (rows, cols)), shape=(N, N), dtype=np.float32)
    deg = np.array(adj.sum(axis=1)).flatten(); deg[deg==0]=1.
    d_is = 1.0/np.sqrt(deg)
    val = d_is[adj.row]*adj.data*d_is[adj.col]
    norm = coo_matrix((val, (adj.row, adj.col)), shape=(N,N), dtype=np.float32)
    idx = torch.tensor(np.vstack([norm.row, norm.col]), dtype=torch.long)
    vals= torch.tensor(norm.data, dtype=torch.float32)
    return torch.sparse_coo_tensor(idx, vals, (N,N)).coalesce().to(device)

norm_adj = build_norm_adj(n_users, n_items, train_df)

# -------------------------------
# 4) LightGCN (grad-safe dense propagation)
# -------------------------------
USE_DENSE_PROP = True  # set False for sparse mm (faster, but can be flaky on some Colab builds)

class LightGCN(nn.Module):
    def __init__(self, n_users, n_items, norm_adj, embed_dim=64, n_layers=3, use_dense=USE_DENSE_PROP):
        super().__init__()
        self.n_users, self.n_items = n_users, n_items
        self.n_layers, self.use_dense = n_layers, use_dense
        self.E = nn.Embedding(n_users + n_items, embed_dim)
        nn.init.xavier_uniform_(self.E.weight)

        if self.use_dense:
            self.register_buffer("A_dense", norm_adj.to_dense())
        else:
            self.register_buffer("A_idx", norm_adj.indices())
            self.register_buffer("A_val", norm_adj.values())
            self.A_shape = norm_adj.shape

    def _mm(self, x):
        if self.use_dense:
            return self.A_dense @ x
        else:
            A = torch.sparse_coo_tensor(self.A_idx, self.A_val, self.A_shape).coalesce()
            return torch.sparse.mm(A, x)

    def propagate(self):
        x = self.E.weight
        embs = [x]
        for _ in range(self.n_layers):
            x = self._mm(x)
            embs.append(x)
        return torch.stack(embs, dim=0).mean(0)

    def get_user_item(self):
        all_out = self.propagate()
        return all_out[:self.n_users], all_out[self.n_users:]

    def score(self, users, items):
        U, I = self.get_user_item()
        return (U[users] * I[items]).sum(dim=1)

model = LightGCN(n_users, n_items, norm_adj, embed_dim=64, n_layers=3).to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)

# -------------------------------
# 5) BPR training + evaluation
# -------------------------------
def sample_bpr_batch(u_pos_dict, n_items, batch_size=4096):
    keys = [u for u,v in u_pos_dict.items() if len(v)>0]
    if not keys:
        return (torch.empty(0,dtype=torch.long),
                torch.empty(0,dtype=torch.long),
                torch.empty(0,dtype=torch.long))
    users, pos_items, neg_items = [], [], []
    for _ in range(batch_size):
        u = random.choice(keys)
        i_pos = random.choice(tuple(u_pos_dict[u]))
        # sample a negative not in user's positives
        while True:
            j = random.randint(0, n_items-1)
            if j not in u_pos_dict[u]: break
        users.append(u); pos_items.append(i_pos); neg_items.append(j)
    return torch.tensor(users), torch.tensor(pos_items), torch.tensor(neg_items)

def bpr_loss(pos_scores, neg_scores):
    return -F.logsigmoid(pos_scores - neg_scores).mean()

@torch.no_grad()
def evaluate(model, train_pos, eval_pos, K_list=(10,20), sample_neg=1000):
    U, I = model.get_user_item()
    metrics = {f"{m}@{k}":[] for k in K_list for m in ["Recall","NDCG","MRR","Hit"]}
    users = sorted(eval_pos.keys())
    all_items = np.arange(n_items)

    for u in users:
        pos_items = list(eval_pos[u])
        if not pos_items: continue

        seen = train_pos.get(u, set())
        neg_pool = np.setdiff1d(all_items, np.array(list(seen)))
        neg_cands = (np.random.choice(neg_pool, size=min(sample_neg, len(neg_pool)), replace=False)
                     if len(neg_pool)>0 else np.array([], dtype=int))
        candidates = np.unique(np.concatenate([neg_cands, np.array(pos_items)]))

        scores = (U[u].unsqueeze(0) * I[candidates]).sum(dim=1).cpu().numpy()
        order = np.argsort(-scores); ranked_items = candidates[order]

        for K in K_list:
            topk = ranked_items[:K]
            hits = [1 if it in pos_items else 0 for it in topk]
            hit_any = 1 if any(hits) else 0
            recall = sum(hits)/len(pos_items)
            dcg = sum(1.0/math.log2(idx+2) for idx,it in enumerate(topk) if it in pos_items)
            idcg = sum(1.0/math.log2(i+2) for i in range(min(len(pos_items),K)))
            ndcg = dcg/idcg if idcg>0 else 0.0
            rr = 0.0
            for idx,it in enumerate(ranked_items):
                if it in pos_items: rr = 1.0/(idx+1); break
            metrics[f"Recall@{K}"].append(recall)
            metrics[f"NDCG@{K}"].append(ndcg)
            metrics[f"MRR@{K}"].append(rr)
            metrics[f"Hit@{K}"].append(hit_any)

    return {m: float(np.mean(v)) if v else 0.0 for m,v in metrics.items()}

@torch.no_grad()
def auc_score(model, train_pos, eval_pos, neg_per_pos=50):
    U, I = model.get_user_item()
    aucs = []
    for u, pos_set in eval_pos.items():
        pos_items = list(pos_set)
        if not pos_items: continue
        seen = train_pos.get(u, set())
        all_negs = [j for j in range(n_items) if j not in seen]
        if len(all_negs)==0: continue
        negs = random.sample(all_negs, min(neg_per_pos, len(all_negs)))

        pos_s = (U[u].unsqueeze(0) * I[pos_items]).sum(dim=1).cpu().numpy()
        neg_s = (U[u].unsqueeze(0) * I[negs]).sum(dim=1).cpu().numpy()
        wins = sum(sp > sn for sp in pos_s for sn in neg_s)
        total = len(pos_s)*len(neg_s)
        if total>0: aucs.append(wins/total)
    return float(np.mean(aucs)) if aucs else 0.0

# ---- NEW: Accuracy & RMSE ----
@torch.no_grad()
def accuracy_score(model, eval_df, threshold=0.5):
    """Accuracy on eval pairs; if no 'rating', assumes all ones (implicit)."""
    U, I = model.get_user_item()
    users = eval_df['u'].values
    items = eval_df['i'].values
    preds = (U[users] * I[items]).sum(dim=1).cpu().numpy()
    preds_norm = (preds - preds.min()) / (preds.max() - preds.min() + 1e-8)
    if 'rating' in eval_df.columns:
        labels = (pd.to_numeric(eval_df['rating'], errors='coerce').fillna(0).values > 0).astype(int)
    else:
        labels = np.ones_like(preds_norm)  # all held-out are positives in LOO
    preds_bin = (preds_norm >= threshold).astype(int)
    return float((preds_bin == labels).mean())

@torch.no_grad()
def rmse_score(model, eval_df):
    """RMSE on eval pairs; if no 'rating', compares to 1.0 (implicit positives)."""
    U, I = model.get_user_item()
    users = eval_df['u'].values
    items = eval_df['i'].values
    preds = (U[users] * I[items]).sum(dim=1).cpu().numpy()
    if 'rating' in eval_df.columns:
        labels = pd.to_numeric(eval_df['rating'], errors='coerce').fillna(0).values.astype(float)
    else:
        labels = np.ones_like(preds, dtype=float)
    preds_scaled = (preds - preds.min()) / (preds.max() - preds.min() + 1e-8) * (labels.max()-labels.min()) + labels.min()
    return float(np.sqrt(np.mean((preds_scaled - labels)**2)))

# -------------------------------
# Train
# -------------------------------
EPOCHS, BATCH_SIZE, eval_every = 50, 4096, 5
print("Start training...")
for epoch in range(1, EPOCHS+1):
    model.train()
    users, pos_items, neg_items = sample_bpr_batch(u_train_pos, n_items, BATCH_SIZE)
    if users.numel()==0:
        print("No training triples available. Check your split/data.")
        break
    users, pos_items, neg_items = users.to(device), pos_items.to(device), neg_items.to(device)

    pos_scores = model.score(users, pos_items)
    neg_scores = model.score(users, neg_items)
    loss = bpr_loss(pos_scores, neg_scores)

    opt.zero_grad(); loss.backward(); opt.step()

    if epoch % eval_every == 0 or epoch == 1:
        model.eval()
        val_metrics = evaluate(model, u_train_pos, u_val_pos, K_list=(10,20))
        val_auc  = auc_score(model, u_train_pos, u_val_pos)
        val_acc  = accuracy_score(model, val_df)
        val_rmse = rmse_score(model, val_df)
        print(f"[Epoch {epoch:03d}] Loss={loss.item():.4f} | "
              f"Val R@10={val_metrics['Recall@10']:.4f} N@10={val_metrics['NDCG@10']:.4f} "
              f"MRR@10={val_metrics['MRR@10']:.4f} Hit@10={val_metrics['Hit@10']:.4f} | "
              f"AUC={val_auc:.4f} Acc={val_acc:.4f} RMSE={val_rmse:.4f}")

# -------------------------------
# Final Test Evaluation
# -------------------------------
print("\nFinal evaluation on TEST:")
model.eval()
test_metrics = evaluate(model, u_train_pos, u_test_pos, K_list=(10,20))
test_auc  = auc_score(model, u_train_pos, u_test_pos)
test_acc  = accuracy_score(model, test_df)
test_rmse = rmse_score(model, test_df)

for k,v in test_metrics.items():
    print(f"{k}: {v:.4f}")
print(f"AUC: {test_auc:.4f}")
print(f"Accuracy: {test_acc:.4f}")
print(f"RMSE: {test_rmse:.4f}")

# -------------------------------
# Final Test Evaluation (Table)
# -------------------------------
print("\nFinal evaluation on TEST for LightGCN Baseline:")
model.eval()
test_metrics = evaluate(model, u_train_pos, u_test_pos, K_list=(10,20))
test_auc  = auc_score(model, u_train_pos, u_test_pos)
test_acc  = accuracy_score(model, test_df)
test_rmse = rmse_score(model, test_df)

# Build results dict
results = {
    "Recall@10": test_metrics['Recall@10'],
    "NDCG@10":   test_metrics['NDCG@10'],
    "MRR@10":    test_metrics['MRR@10'],
    "Hit@10":    test_metrics['Hit@10'],
    "Recall@20": test_metrics['Recall@20'],
    "NDCG@20":   test_metrics['NDCG@20'],
    "MRR@20":    test_metrics['MRR@20'],
    "Hit@20":    test_metrics['Hit@20'],
    "AUC":       test_auc,
    "Accuracy":  test_acc,
    "RMSE":      test_rmse
}

# Pretty table with pandas
import pandas as pd
df_results = pd.DataFrame([results])
print(df_results.round(4).to_string(index=False))