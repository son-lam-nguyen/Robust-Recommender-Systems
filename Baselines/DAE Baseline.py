# -*- coding: utf-8 -*-
"""New 2 - DAE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w1XGrBKWKA2iEa5I1_g6d4B8Rmjrr1Gb
"""

# ============================================
# Baseline DAE (base, tied weights) â€” Colab-ready
# - Upload CSV/XLS/XLSX
# - Leave-one-out split (time-aware if timestamp)
# - Train DAE with weighted BCE (implicit) + input corruption
# - Metrics: Recall@K, NDCG@K, MRR, Hit@K, AUC, Accuracy, RMSE
# - Final results printed in a table
# ============================================

!pip install -q pandas numpy scipy torch==2.3.1 tqdm openpyxl

import os, math, random, numpy as np, pandas as pd
from collections import defaultdict
from tqdm import tqdm
import torch, torch.nn as nn
import torch.nn.functional as F
from scipy.sparse import csr_matrix
from google.colab import files

# -------------------------------
# Repro / device
# -------------------------------
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Device:", device)

# -------------------------------
# Upload & Load Dataset1
# -------------------------------
uploaded = files.upload()
INPUT_FILE = list(uploaded.keys())[0]
print("Using:", INPUT_FILE)

def load_any(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in ['.csv', '.txt']:
        return pd.read_csv(path)
    elif ext in ['.xls', '.xlsx']:
        return pd.read_excel(path)
    raise ValueError("Upload CSV/XLS/XLSX.")

df_raw = load_any(INPUT_FILE)
print("Columns:", list(df_raw.columns)); print(df_raw.head())

# -------------------------------
# Standardize columns
# -------------------------------
def guess_col(df, groups):
    low = {c.lower(): c for c in df.columns}
    for g in groups:
        for c in g:
            if c.lower() in low:
                return low[c.lower()]
    return None

user_col = guess_col(df_raw, [['user','user_id','uid','userid','User','UserID','UID']])
item_col = guess_col(df_raw, [['item','item_id','iid','movieId','product_id','Item','ItemID','IID']])
rating_col = guess_col(df_raw, [['rating','ratings','score','rate','Score']])
ts_col     = guess_col(df_raw, [['timestamp','time','ts','date','datetime','created_at']])

if user_col is None or item_col is None:
    user_col, item_col = df_raw.columns[:2]
    print("Fallback to first two columns as (user,item):", user_col, item_col)

cols = [user_col, item_col] + ([rating_col] if rating_col else []) + ([ts_col] if ts_col else [])
df = df_raw[cols].copy()
df.rename(columns={user_col:'user', item_col:'item'}, inplace=True)
if rating_col: df.rename(columns={rating_col:'rating'}, inplace=True)
if ts_col:     df.rename(columns={ts_col:'timestamp'}, inplace=True)

# Implicit binarization for training targets
if 'rating' in df.columns:
    df['y'] = (pd.to_numeric(df['rating'], errors='coerce').fillna(0) > 0).astype(int)
else:
    df['y'] = 1
df = df[df['y'] > 0]

# Map to contiguous ids
df['user'] = df['user'].astype(str)
df['item'] = df['item'].astype(str)
user2id = {u:i for i,u in enumerate(df['user'].astype('category').cat.categories)}
item2id = {v:i for i,v in enumerate(df['item'].astype('category').cat.categories)}
df['u'] = df['user'].map(user2id)
df['i'] = df['item'].map(item2id)
n_users, n_items = len(user2id), len(item2id)
print(f"#Users={n_users}, #Items={n_items}, #Interactions={len(df)}")

# -------------------------------
# Leave-One-Out split (keep rating for eval if available)
# -------------------------------
def leave_one_out_split(df_ui):
    frames = []
    if 'timestamp' in df_ui.columns:
        for u, g in df_ui.groupby('u'):
            frames.append(g.sort_values('timestamp'))
    else:
        for u, g in df_ui.groupby('u'):
            frames.append(g.sample(frac=1.0, random_state=SEED))
    train, val, test = [], [], []
    for g in frames:
        if len(g)==1: test.append(g.iloc[-1])
        elif len(g)==2: val.append(g.iloc[-2]); test.append(g.iloc[-1])
        else:
            train.extend(list(g.iloc[:-2].itertuples(index=False)))
            val.append(g.iloc[-2]); test.append(g.iloc[-1])
    cols = df_ui.columns
    train = pd.DataFrame(train, columns=cols) if train else df_ui.head(0)
    val   = pd.DataFrame(val,   columns=cols)
    test  = pd.DataFrame(test,  columns=cols)
    return train.reset_index(drop=True), val.reset_index(drop=True), test.reset_index(drop=True)

base_cols = ['u','i'] + (['timestamp'] if 'timestamp' in df.columns else []) + (['rating'] if 'rating' in df.columns else [])
train_df, val_df, test_df = leave_one_out_split(df[base_cols])
print("#Train:", len(train_df), "#Val:", len(val_df), "#Test:", len(test_df))

def u2pos(df_):
    d = defaultdict(set)
    for u,i in zip(df_['u'].values, df_['i'].values):
        d[u].add(i)
    return d

u_train_pos = u2pos(train_df)
u_val_pos   = u2pos(val_df)
u_test_pos  = u2pos(test_df)

# Build CSR user-item matrix for TRAIN (dense mini-batch creation)
rows, cols = [], []
for u,i in zip(train_df['u'].values, train_df['i'].values):
    rows.append(u); cols.append(i)
train_csr = csr_matrix((np.ones(len(rows), dtype=np.float32),
                        (np.array(rows), np.array(cols))),
                       shape=(n_users, n_items), dtype=np.float32)

# -------------------------------
# DAE (base) with tied weights
#  - Single hidden layer (n_items -> h -> n_items)
#  - Decoder weight is enc.weight^T (only bias is learnable)
# -------------------------------
class DAE_Tied(nn.Module):
    def __init__(self, n_items, hidden=600, dropout=0.3):
        super().__init__()
        self.hidden = hidden
        self.enc = nn.Linear(n_items, hidden, bias=True)
        nn.init.xavier_uniform_(self.enc.weight)
        self.dec_bias = nn.Parameter(torch.zeros(n_items))
        self.dropout = nn.Dropout(dropout)
        self.act = nn.ReLU()

    def forward(self, x):
        x = self.dropout(x)
        z = self.act(self.enc(x))
        # tied decoder: logits = z @ W^T + b
        logits = F.linear(z, self.enc.weight.t(), self.dec_bias)
        return logits

model = DAE_Tied(n_items, hidden=600, dropout=0.3).to(device)

# Weighted BCE for implicit data (positives upweighted)
alpha = 5.0
def weighted_bce_with_logits(logits, target, alpha=1.0):
    loss_pos = F.logsigmoid(logits)
    loss_neg = F.logsigmoid(-logits)
    loss = -(alpha * target * loss_pos + (1 - target) * loss_neg)
    return loss.mean()

opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)

# -------------------------------
# Training utilities
# -------------------------------
def build_batch_from_csr(user_ids):
    B = len(user_ids)
    X = torch.zeros(B, n_items, dtype=torch.float32)
    for r, u in enumerate(user_ids):
        start, end = train_csr.indptr[u], train_csr.indptr[u+1]
        items = train_csr.indices[start:end]
        if len(items) > 0:
            X[r, items] = 1.0
    return X

def iterate_minibatches(batch_size=256, shuffle=True):
    users = np.arange(n_users)
    if shuffle:
        np.random.shuffle(users)
    for b in range(0, n_users, batch_size):
        yield users[b:b+batch_size]

# Denoising corruption: randomly drop some observed 1s
def corrupt_input(X, drop_prob=0.3):
    if drop_prob <= 0:
        return X
    noise = torch.rand_like(X)
    mask = torch.ones_like(X)
    mask[(X > 0) & (noise < drop_prob)] = 0.0
    return X * mask

# -------------------------------
# Evaluation helpers (same style as other baselines)
# -------------------------------
@torch.no_grad()
def get_user_scores(model, user_id):
    x = build_batch_from_csr([user_id]).to(device)
    logits = model(x)[0]
    return logits.detach().cpu().numpy()

@torch.no_grad()
def evaluate(model, train_pos, eval_pos, K_list=(10,20), sample_neg=1000):
    metrics = {f"{m}@{k}":[] for k in K_list for m in ["Recall","NDCG","MRR","Hit"]}
    users = sorted(eval_pos.keys())
    all_items = np.arange(n_items)

    for u in users:
        pos_items = list(eval_pos[u])
        if not pos_items: continue

        seen = train_pos.get(u, set())
        neg_pool = np.setdiff1d(all_items, np.array(list(seen)))
        neg_cands = (np.random.choice(neg_pool, size=min(sample_neg, len(neg_pool)), replace=False)
                     if len(neg_pool)>0 else np.array([], dtype=int))
        candidates = np.unique(np.concatenate([neg_cands, np.array(pos_items)]))

        scores = get_user_scores(model, u)[candidates]
        order = np.argsort(-scores); ranked_items = candidates[order]

        for K in K_list:
            topk = ranked_items[:K]
            hits = [1 if it in pos_items else 0 for it in topk]
            hit_any = 1 if any(hits) else 0
            recall = sum(hits)/len(pos_items)
            dcg = sum(1.0/math.log2(idx+2) for idx,it in enumerate(topk) if it in pos_items)
            idcg = sum(1.0/math.log2(i+2) for i in range(min(len(pos_items),K)))
            ndcg = dcg/idcg if idcg>0 else 0.0
            rr = 0.0
            for idx,it in enumerate(ranked_items):
                if it in pos_items: rr = 1.0/(idx+1); break
            metrics[f"Recall@{K}"].append(recall)
            metrics[f"NDCG@{K}"].append(ndcg)
            metrics[f"MRR@{K}"].append(rr)
            metrics[f"Hit@{K}"].append(hit_any)

    return {m: float(np.mean(v)) if v else 0.0 for m,v in metrics.items()}

@torch.no_grad()
def auc_score(model, train_pos, eval_pos, neg_per_pos=50):
    aucs = []
    for u, pos_set in eval_pos.items():
        pos_items = list(pos_set)
        if not pos_items: continue
        seen = train_pos.get(u, set())
        all_negs = [j for j in range(n_items) if j not in seen]
        if len(all_negs)==0: continue
        negs = random.sample(all_negs, min(neg_per_pos, len(all_negs)))
        scores = get_user_scores(model, u)
        pos_s = scores[pos_items]; neg_s = scores[negs]
        wins = sum(sp > sn for sp in pos_s for sn in neg_s)
        total = len(pos_s)*len(neg_s)
        if total>0: aucs.append(wins/total)
    return float(np.mean(aucs)) if aucs else 0.0

@torch.no_grad()
def accuracy_score(model, eval_df, threshold=0.5):
    users = eval_df['u'].values
    items = eval_df['i'].values
    preds = np.array([get_user_scores(model, u)[i] for u,i in zip(users, items)])
    preds_norm = (preds - preds.min()) / (preds.max() - preds.min() + 1e-8)
    if 'rating' in eval_df.columns:
        labels = (pd.to_numeric(eval_df['rating'], errors='coerce').fillna(0).values > 0).astype(int)
    else:
        labels = np.ones_like(preds_norm)
    preds_bin = (preds_norm >= threshold).astype(int)
    return float((preds_bin == labels).mean())

@torch.no_grad()
def rmse_score(model, eval_df):
    users = eval_df['u'].values
    items = eval_df['i'].values
    preds = np.array([get_user_scores(model, u)[i] for u,i in zip(users, items)])
    if 'rating' in eval_df.columns:
        labels = pd.to_numeric(eval_df['rating'], errors='coerce').fillna(0).values.astype(float)
    else:
        labels = np.ones_like(preds, dtype=float)
    preds_scaled = (preds - preds.min()) / (preds.max() - preds.min() + 1e-8) * (labels.max()-labels.min()) + labels.min()
    return float(np.sqrt(np.mean((preds_scaled - labels)**2)))

# -------------------------------
# Train DAE (base)
# -------------------------------
EPOCHS = 80
BATCH_SIZE = 256
drop_prob = 0.3       # corruption probability
eval_every = 5

print("Start training DAE (base, tied weights)...")
for epoch in range(1, EPOCHS+1):
    model.train()
    losses = []
    for user_batch in iterate_minibatches(BATCH_SIZE, shuffle=True):
        X = build_batch_from_csr(user_batch).to(device)          # (B, n_items)
        X_in = corrupt_input(X, drop_prob=drop_prob).to(device)  # denoising
        logits = model(X_in)
        loss = weighted_bce_with_logits(logits, X, alpha=alpha)

        opt.zero_grad()
        loss.backward()
        opt.step()
        losses.append(loss.item())

    if epoch % eval_every == 0 or epoch == 1:
        model.eval()
        val_metrics = evaluate(model, u_train_pos, u_val_pos, K_list=(10,20))
        val_auc  = auc_score(model, u_train_pos, u_val_pos)
        val_acc  = accuracy_score(model, val_df)
        val_rmse = rmse_score(model, val_df)
        print(f"[Epoch {epoch:03d}] TrainLoss={np.mean(losses):.4f} | "
              f"Val R@10={val_metrics['Recall@10']:.4f} N@10={val_metrics['NDCG@10']:.4f} "
              f"MRR@10={val_metrics['MRR@10']:.4f} Hit@10={val_metrics['Hit@10']:.4f} | "
              f"AUC={val_auc:.4f} Acc={val_acc:.4f} RMSE={val_rmse:.4f}")

# -------------------------------
# Final Test Evaluation (Table)
# -------------------------------
print("\nFinal evaluation on TEST:")
model.eval()
test_metrics = evaluate(model, u_train_pos, u_test_pos, K_list=(10,20))
test_auc  = auc_score(model, u_train_pos, u_test_pos)
test_acc  = accuracy_score(model, test_df)
test_rmse = rmse_score(model, test_df)

results = {
    "Recall@10": test_metrics['Recall@10'],
    "NDCG@10":   test_metrics['NDCG@10'],
    "MRR@10":    test_metrics['MRR@10'],
    "Hit@10":    test_metrics['Hit@10'],
    "Recall@20": test_metrics['Recall@20'],
    "NDCG@20":   test_metrics['NDCG@20'],
    "MRR@20":    test_metrics['MRR@20'],
    "Hit@20":    test_metrics['Hit@20'],
    "AUC":       test_auc,
    "Accuracy":  test_acc,
    "RMSE":      test_rmse
}
print(pd.DataFrame([results]).round(4).to_string(index=False))