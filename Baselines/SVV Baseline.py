# -*- coding: utf-8 -*-
"""New 2 - SVV Only.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aji49Faapw-5BGqLoGNpvFwI4SDqL2Pk
"""

# ======================================================
# SVV-Only Baseline (Colab-ready)
# Shapley Value-driven Valuation (SVV) to prune noisy interactions
# Steps:
#  1. Train Denoising Autoencoder (DAE)
#  2. Approximate Shapley values for user–item pairs
#  3. Prune low-value interactions (S_clean)
#  4. Retrain LightGCN on S_clean
# ======================================================

!pip install -q pandas numpy scipy torch==2.3.1 tqdm openpyxl

import os, random, math, numpy as np, pandas as pd
import torch, torch.nn as nn, torch.nn.functional as F
from tqdm import tqdm
from collections import defaultdict
from google.colab import files

# -----------------------
# Setup
# -----------------------
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# -----------------------
# Load Dataset
# -----------------------
uploaded = files.upload()
INPUT_FILE = list(uploaded.keys())[0]
print("Using:", INPUT_FILE)

def load_any(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in [".csv", ".txt"]:
        return pd.read_csv(path)
    elif ext in [".xls", ".xlsx"]:
        return pd.read_excel(path)
    else:
        raise ValueError("Upload CSV/XLS/XLSX")

df = load_any(INPUT_FILE)
print(df.head())

# Standardize columns
user_col = [c for c in df.columns if "user" in c.lower()][0]
item_col = [c for c in df.columns if "item" in c.lower()][0]
df.rename(columns={user_col: "user", item_col: "item"}, inplace=True)
df["user"] = df["user"].astype(str)
df["item"] = df["item"].astype(str)

# Map to IDs
user2id = {u:i for i,u in enumerate(df["user"].astype("category").cat.categories)}
item2id = {v:i for i,v in enumerate(df["item"].astype("category").cat.categories)}
df["u"] = df["user"].map(user2id)
df["i"] = df["item"].map(item2id)
n_users, n_items = len(user2id), len(item2id)
print(f"#Users={n_users}, #Items={n_items}, #Interactions={len(df)}")

# -----------------------
# Build User–Item Matrix
# -----------------------
R = torch.zeros((n_users, n_items))
for u, i in zip(df["u"], df["i"]):
    R[u, i] = 1.0

# -----------------------
# Denoising Autoencoder (DAE)
# -----------------------
class DAE(nn.Module):
    def __init__(self, n_items, hidden_dim=256):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(n_items, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim//2)
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim//2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, n_items),
            nn.Sigmoid()
        )
    def forward(self, x):
        z = self.encoder(x)
        out = self.decoder(z)
        return out, z

dae = DAE(n_items).to(device)
opt = torch.optim.Adam(dae.parameters(), lr=1e-3)
criterion = nn.MSELoss()

# -----------------------
# Train DAE
# -----------------------
EPOCHS = 30
BATCH_SIZE = 256
dae.train()
for epoch in range(EPOCHS):
    total_loss = 0
    idx = np.arange(n_users)
    np.random.shuffle(idx)
    for start in range(0, len(idx), BATCH_SIZE):
        batch = idx[start:start+BATCH_SIZE]
        x = R[batch].to(device)
        x_noisy = x * (torch.rand_like(x) > 0.1).float()  # dropout noise
        out, _ = dae(x_noisy)
        loss = criterion(out, x)
        opt.zero_grad(); loss.backward(); opt.step()
        total_loss += loss.item()
    if (epoch+1) % 5 == 0:
        print(f"Epoch {epoch+1}/{EPOCHS} | Loss={total_loss/len(idx):.6f}")

# -----------------------
# Approximate Shapley Values (FastSHAP-like)
# -----------------------
dae.eval()
with torch.no_grad():
    R_rec, Z = dae(R.to(device))
    errors = (R.to(device) - R_rec).abs()

# Influence score per interaction = 1 - reconstruction error
shapley_scores = 1.0 - errors.cpu().numpy()
df["shapley_value"] = [shapley_scores[u, i] for u, i in zip(df["u"], df["i"])]

print("Shapley value range:", df["shapley_value"].min(), "→", df["shapley_value"].max())

# -----------------------
# Prune Low-Contribution Interactions
# -----------------------
threshold = df["shapley_value"].quantile(0.2)  # keep top 80%
df_clean = df[df["shapley_value"] >= threshold].reset_index(drop=True)
print(f"After pruning: {len(df_clean)} interactions (kept 80%)")

def propagate(self, edge_index, user_emb, item_emb):
    emb = torch.cat([user_emb, item_emb], dim=0)
    num_nodes = emb.size(0)

    # Build adjacency matrix
    row, col = edge_index
    A = torch.sparse_coo_tensor(
        torch.stack([row, col]),
        torch.ones(len(row), device=edge_index.device),
        (num_nodes, num_nodes)
    )

    deg = torch.sparse.sum(A, dim=1).to_dense()
    deg_inv_sqrt = torch.pow(deg, -0.5)
    deg_inv_sqrt[torch.isinf(deg_inv_sqrt)] = 0
    D_inv_sqrt = deg_inv_sqrt

    norm_values = D_inv_sqrt[row] * D_inv_sqrt[col]
    A_norm = torch.sparse_coo_tensor(
        torch.stack([row, col]),
        norm_values,
        (num_nodes, num_nodes)
    )

    # Propagate
    all_emb = [emb]
    for _ in range(self.n_layers):
        emb = torch.sparse.mm(A_norm, emb)
        all_emb.append(emb)
    out = torch.stack(all_emb, dim=1).mean(1)
    return out[:self.user_emb.num_embeddings], out[self.user_emb.num_embeddings:]

!pip install torch-scatter torch-sparse torch-geometric -f https://data.pyg.org/whl/torch-2.3.1+cu121.html

# -----------------------
# Train LightGCN on S_clean
# -----------------------
import torch_sparse
from torch.utils.data import DataLoader

# Build edge list
edges = torch.tensor(df_clean[["u","i"]].values.T, dtype=torch.long).to(device)
edge_index = torch.cat([edges, edges.flip(0) + torch.tensor([[0],[n_users]], device=device)], dim=1)

class LightGCN(nn.Module):
    def __init__(self, n_users, n_items, emb_dim=64, n_layers=3):
        super().__init__()
        self.user_emb = nn.Embedding(n_users, emb_dim)
        self.item_emb = nn.Embedding(n_items, emb_dim)
        self.n_layers = n_layers
        nn.init.xavier_uniform_(self.user_emb.weight)
        nn.init.xavier_uniform_(self.item_emb.weight)

    def propagate(self, edge_index, user_emb, item_emb):
        emb = torch.cat([user_emb, item_emb], dim=0)
        deg = torch.bincount(edge_index[0])
        deg_inv_sqrt = (deg.float().pow(-0.5))
        deg_inv_sqrt[deg_inv_sqrt == float("inf")] = 0
        norm = deg_inv_sqrt[edge_index[0]] * deg_inv_sqrt[edge_index[1]]
        for _ in range(self.n_layers):
            emb = emb + torch_sparse.spmm(edge_index, norm, emb.size(0), emb.size(0), emb)
        return emb[:n_users], emb[n_users:]

    def forward(self, users, pos_items, neg_items, edge_index):
        u_emb, i_emb = self.user_emb.weight, self.item_emb.weight
        u_emb, i_emb = self.propagate(edge_index, u_emb, i_emb)
        u_vec = u_emb[users]; i_pos = i_emb[pos_items]; i_neg = i_emb[neg_items]
        pos_score = (u_vec * i_pos).sum(dim=1)
        neg_score = (u_vec * i_neg).sum(dim=1)
        loss = -torch.mean(F.logsigmoid(pos_score - neg_score))
        return loss

def sample_bpr_batch(df, n_users, n_items, batch_size=1024):
    users, pos_items, neg_items = [], [], []
    u2items = df.groupby("u")["i"].apply(set).to_dict()
    for _ in range(batch_size):
        u = random.randint(0, n_users-1)
        if u not in u2items: continue
        i_pos = random.choice(list(u2items[u]))
        while True:
            j = random.randint(0, n_items-1)
            if j not in u2items[u]: break
        users.append(u); pos_items.append(i_pos); neg_items.append(j)
    return torch.tensor(users), torch.tensor(pos_items), torch.tensor(neg_items)

# Train LightGCN
model = LightGCN(n_users, n_items).to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3)
EPOCHS = 30
for epoch in range(1, EPOCHS+1):
    model.train()
    u, pi, ni = sample_bpr_batch(df_clean, n_users, n_items)
    u, pi, ni = u.to(device), pi.to(device), ni.to(device)
    loss = model(u, pi, ni, edge_index)
    opt.zero_grad(); loss.backward(); opt.step()
    if epoch % 5 == 0:
        print(f"Epoch {epoch}/{EPOCHS}, Loss={loss.item():.4f}")

# -----------------------
# Evaluate
# -----------------------
@torch.no_grad()
def evaluate_full(model, df_clean, n_users, n_items, edge_index):
    # Compute embeddings
    u_emb, i_emb = model.user_emb.weight, model.item_emb.weight
    u_emb, i_emb = model.propagate(edge_index, u_emb, i_emb)

    results = {
        "Recall@10": [], "Recall@20": [],
        "NDCG@10": [], "NDCG@20": [],
        "MRR@10": [], "MRR@20": []
    }

    for u in range(n_users):
        pos = set(df_clean[df_clean["u"] == u]["i"].tolist())
        if not pos:
            continue

        scores = (u_emb[u] @ i_emb.T).cpu().numpy()
        ranked = np.argsort(-scores)

        for K in [10, 20]:
            topk = ranked[:K]
            hits = [1 if i in pos else 0 for i in topk]
            recall = np.mean(hits)
            dcg = sum(h / np.log2(idx + 2) for idx, h in enumerate(hits))
            idcg = sum(1 / np.log2(i + 2) for i in range(min(len(pos), K)))
            ndcg = dcg / idcg if idcg > 0 else 0
            rr = 0.0
            for idx, it in enumerate(ranked):
                if it in pos:
                    rr = 1.0 / (idx + 1)
                    break
            results[f"Recall@{K}"].append(recall)
            results[f"NDCG@{K}"].append(ndcg)
            results[f"MRR@{K}"].append(rr)

    # Average metrics
    avg = {m: np.mean(v) for m, v in results.items()}
    return avg

@torch.no_grad()
def accuracy_score(model, df_clean, edge_index, threshold=0.5):
    u_emb, i_emb = model.user_emb.weight, model.item_emb.weight
    u_emb, i_emb = model.propagate(edge_index, u_emb, i_emb)
    preds, labels = [], []
    for _, row in df_clean.iterrows():
        u, i = int(row.u), int(row.i)
        preds.append((u_emb[u] @ i_emb[i]).item())
        labels.append(1.0)
    preds = np.array(preds)
    preds_norm = (preds - preds.min()) / (preds.max() - preds.min() + 1e-8)
    pred_bin = (preds_norm >= threshold).astype(int)
    acc = (pred_bin == 1).mean()
    return acc

@torch.no_grad()
def auc_score(model, df_clean, edge_index):
    u_emb, i_emb = model.user_emb.weight, model.item_emb.weight
    u_emb, i_emb = model.propagate(edge_index, u_emb, i_emb)
    aucs = []
    for u in range(n_users):
        pos = set(df_clean[df_clean["u"] == u]["i"].tolist())
        if not pos:
            continue
        negs = list(set(range(n_items)) - pos)
        if len(negs) == 0:
            continue
        pos_s = [(u_emb[u] @ i_emb[i]).item() for i in pos]
        neg_s = [(u_emb[u] @ i_emb[j]).item() for j in random.sample(negs, min(50, len(negs)))]
        wins = sum(p > n for p in pos_s for n in neg_s)
        aucs.append(wins / (len(pos_s) * len(neg_s)))
    return float(np.mean(aucs))

@torch.no_grad()
def rmse_score(model, df_clean, edge_index):
    u_emb, i_emb = model.user_emb.weight, model.item_emb.weight
    u_emb, i_emb = model.propagate(edge_index, u_emb, i_emb)
    preds = [(u_emb[int(r.u)] @ i_emb[int(r.i)]).item() for _, r in df_clean.iterrows()]
    labels = np.ones(len(preds))
    preds = np.array(preds)
    preds_norm = (preds - preds.min()) / (preds.max() - preds.min() + 1e-8)
    rmse = np.sqrt(np.mean((preds_norm - labels) ** 2))
    return rmse

# -----------------------
# Final Evaluation on S_clean
# -----------------------
print("\nFinal Evaluation on S_clean Dataset:")

metrics = evaluate_full(model, df_clean, n_users, n_items, edge_index)
acc = accuracy_score(model, df_clean, edge_index)
auc = auc_score(model, df_clean, edge_index)
rmse = rmse_score(model, df_clean, edge_index)

import pandas as pd
result_table = pd.DataFrame([{
    "Method": "SVV Only",
    "Recall@10": metrics["Recall@10"],
    "Recall@20": metrics["Recall@20"],
    "NDCG@10": metrics["NDCG@10"],
    "NDCG@20": metrics["NDCG@20"],
    "MRR@10": metrics["MRR@10"],
    "MRR@20": metrics["MRR@20"],
    "Accuracy": acc,
    "AUC": auc,
    "RMSE": rmse
}]).round(4)

print(result_table.to_string(index=False))