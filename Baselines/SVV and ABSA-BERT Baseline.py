# -*- coding: utf-8 -*-
"""New 2-SVV + ABSA-BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zTsa8csm9CgVcztYokDVrB9JWd-OFMw7
"""

# ==========================================================
# SVV + ABSA-BERT (Ours) — Colab-ready
# 1) DAE -> Shapley-like scores -> prune noisy interactions (SVV)
# 2) Use sentiment score (from dataset's columns if available) (ABSA-BERT)
# 3) Train sentiment-aware BPRMF on S_clean
# 4) Report full metrics table on TEST
# ==========================================================

!pip install -q pandas numpy scipy torch==2.3.1 tqdm openpyxl

import os, math, random, numpy as np, pandas as pd
from collections import defaultdict
from tqdm import tqdm
import torch, torch.nn as nn
import torch.nn.functional as F
from google.colab import files

# ---------------------------
# Repro / device
# ---------------------------
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ---------------------------
# Load Dataset
# ---------------------------
uploaded = files.upload()
INPUT_FILE = list(uploaded.keys())[0]
print("Using:", INPUT_FILE)

def load_any(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in [".csv", ".txt"]:
        return pd.read_csv(path)
    elif ext in [".xls", ".xlsx"]:
        return pd.read_excel(path)
    else:
        raise ValueError("Upload CSV/XLS/XLSX")

df_raw = load_any(INPUT_FILE)
print("Columns:", list(df_raw.columns)[:20])

# ---------------------------
# Standardize columns
# ---------------------------
def pick(df, names):
    low = {c.lower(): c for c in df.columns}
    for n in names:
        if n.lower() in low: return low[n.lower()]
    return None

user_col = pick(df_raw, ["user_id","user","uid"])
item_col = pick(df_raw, ["item_id","item","iid","product_id","movieid"])
rating_col = pick(df_raw, ["rating","score","stars"])
ts_col     = pick(df_raw, ["timestamp","time","unixreviewtime","date","datetime"])
text_col   = pick(df_raw, ["reviewtext","review_text","review","text"])

# sentiment columns (use if present)
sent_score_col   = pick(df_raw, ["sentiment","sentiment_score"])
sent_label_col   = pick(df_raw, ["sentiment_","sentiment_label"])
sent_numeric_col = pick(df_raw, ["sentiment_numeric","polarity"])

if user_col is None or item_col is None:
    raise ValueError("Couldn't find user/item columns.")

# Fallbacks: if no review or sentiment, create neutral placeholders
if text_col is None:
    df_raw["reviewText"] = ""
    text_col = "reviewText"
if sent_score_col is None:
    print("⚠️ No sentiment score column found; using neutral zeros.")
    df_raw["sentiment"] = 0.0
    sent_score_col = "sentiment"
if sent_label_col is None:
    df_raw["sentiment_"] = "neutral"
    sent_label_col = "sentiment_"
if sent_numeric_col is None:
    df_raw["sentiment_numeric"] = 0
    sent_numeric_col = "sentiment_numeric"

cols = [user_col, item_col, text_col, sent_score_col, sent_label_col, sent_numeric_col] \
       + ([rating_col] if rating_col else []) + ([ts_col] if ts_col else [])
df = df_raw[cols].copy()
df.columns = ["user","item","review","sentiment_score","sentiment_label","sentiment_numeric"] \
             + (["rating"] if rating_col else []) + (["timestamp"] if ts_col else [])

# Implicit positives
if "rating" in df.columns:
    df["y"] = (pd.to_numeric(df["rating"], errors="coerce").fillna(0) > 0).astype(int)
else:
    df["y"] = 1
df = df[df["y"] > 0].copy()

# Map ids
df["user"] = df["user"].astype(str)
df["item"] = df["item"].astype(str)
user2id = {u:i for i,u in enumerate(df["user"].astype("category").cat.categories)}
item2id = {v:i for i,v in enumerate(df["item"].astype("category").cat.categories)}
df["u"] = df["user"].map(user2id)
df["i"] = df["item"].map(item2id)
n_users, n_items = len(user2id), len(item2id)
print(f"#Users={n_users}, #Items={n_items}, #Interactions={len(df)}")

# ---------------------------
# Leave-One-Out Split (keep review+sentiment)
# ---------------------------
def leave_one_out_split(df_ui):
    frames=[]
    if "timestamp" in df_ui.columns:
        for _,g in df_ui.groupby("u"):
            frames.append(g.sort_values("timestamp"))
    else:
        for _,g in df_ui.groupby("u"):
            frames.append(g.sample(frac=1.0, random_state=SEED))
    train,val,test=[],[],[]
    for g in frames:
        if len(g)==1: test.append(g.iloc[-1])
        elif len(g)==2: val.append(g.iloc[-2]); test.append(g.iloc[-1])
        else:
            train.extend(list(g.iloc[:-2].itertuples(index=False)))
            val.append(g.iloc[-2]); test.append(g.iloc[-1])
    cols=df_ui.columns
    train=pd.DataFrame(train, columns=cols) if train else df_ui.head(0)
    val  =pd.DataFrame(val,   columns=cols)
    test =pd.DataFrame(test,  columns=cols)
    return train.reset_index(drop=True), val.reset_index(drop=True), test.reset_index(drop=True)

keep_cols = ["u","i","review","sentiment_score","sentiment_label","sentiment_numeric"] \
            + (["rating"] if "rating" in df.columns else []) \
            + (["timestamp"] if "timestamp" in df.columns else [])
train_df, val_df, test_df = leave_one_out_split(df[keep_cols])
print("#Train:", len(train_df), "#Val:", len(val_df), "#Test:", len(test_df))

# Positive sets (for sampling/eval)
def u2pos(df_):
    d=defaultdict(set)
    for u,i in zip(df_["u"].values, df_["i"].values): d[u].add(i)
    return d
u_train_pos=u2pos(train_df); u_val_pos=u2pos(val_df); u_test_pos=u2pos(test_df)

# ---------------------------
# 1) SVV: DAE -> Shapley-like value -> prune
# ---------------------------
# Build dense user-item train matrix
import scipy.sparse as sp
rows, cols_ = train_df["u"].values, train_df["i"].values
R = sp.csr_matrix((np.ones_like(rows, dtype=np.float32), (rows, cols_)),
                  shape=(n_users, n_items), dtype=np.float32)

class DAE_Tied(nn.Module):
    # simple, fast DAE with tied weights; logits->sigmoid
    def __init__(self, n_items, hidden=600, dropout=0.3):
        super().__init__()
        self.enc = nn.Linear(n_items, hidden, bias=True)
        nn.init.xavier_uniform_(self.enc.weight)
        self.dec_bias = nn.Parameter(torch.zeros(n_items))
        self.dropout = nn.Dropout(dropout)
        self.act = nn.ReLU()
    def forward(self, x):
        x = self.dropout(x)
        z = self.act(self.enc(x))
        logits = F.linear(z, self.enc.weight.t(), self.dec_bias)
        return logits

dae = DAE_Tied(n_items, hidden=600, dropout=0.3).to(device)
opt_dae = torch.optim.Adam(dae.parameters(), lr=1e-3, weight_decay=1e-6)

def iter_user_batches(B=256, shuffle=True):
    idx = np.arange(n_users);
    if shuffle: np.random.shuffle(idx)
    for s in range(0, len(idx), B):
        yield idx[s:s+B]

def build_batch(user_ids):
    X = torch.zeros(len(user_ids), n_items, dtype=torch.float32)
    for r,u in enumerate(user_ids):
        start, end = R.indptr[u], R.indptr[u+1]
        X[r, R.indices[start:end]] = 1.0
    return X

alpha = 5.0
def weighted_bce_with_logits(logits, target, alpha=1.0):
    # pos upweighted
    loss_pos = F.logsigmoid(logits)
    loss_neg = F.logsigmoid(-logits)
    return (-(alpha*target*loss_pos + (1-target)*loss_neg)).mean()

EPOCHS_DAE = 40
drop_prob = 0.3
print("Training DAE for SVV...")
for ep in range(1, EPOCHS_DAE+1):
    dae.train()
    losses=[]
    for ub in iter_user_batches(256, True):
        X = build_batch(ub).to(device)
        # denoising: drop some 1's
        noise = torch.rand_like(X)
        X_in = X.clone()
        X_in[(X>0) & (noise < drop_prob)] = 0.0
        logits = dae(X_in)
        loss = weighted_bce_with_logits(logits, X, alpha=alpha)
        opt_dae.zero_grad(); loss.backward(); opt_dae.step()
        losses.append(loss.item())
    if ep % 5 == 0 or ep == 1:
        print(f"[DAE Ep {ep}] loss={np.mean(losses):.4f}")

# Shapley-like contribution ≈ 1 - reconstruction error on positives
print("Computing Shapley-like scores...")
dae.eval()
with torch.no_grad():
    all_scores = []
    for ub in iter_user_batches(256, False):
        X = build_batch(ub).to(device)
        logits = dae(X)
        prob = torch.sigmoid(logits)
        all_scores.append(prob.cpu())
    Pfull = torch.cat(all_scores, dim=0).numpy()  # (n_users, n_items)

train_df["shapley_value"] = [Pfull[u, i] for u,i in zip(train_df["u"], train_df["i"])]

# Prune bottom q portion (keep top (1-q))
PRUNE_RATE = 0.20   # drop 20% lowest
thr = train_df["shapley_value"].quantile(PRUNE_RATE)
train_clean = train_df[train_df["shapley_value"] > thr].reset_index(drop=True)
print(f"Pruning done: kept {len(train_clean)}/{len(train_df)} = {len(train_clean)/len(train_df):.2%}")

# Update positive sets for training backbone
u_train_pos_clean = u2pos(train_clean)

# ---------------------------
# 2) Sentiment-aware BPR-MF backbone
# ---------------------------
class BPRMF_Sent(nn.Module):
    def __init__(self, n_users, n_items, dim=64):
        super().__init__()
        self.U = nn.Embedding(n_users, dim)
        self.I = nn.Embedding(n_items, dim)
        nn.init.xavier_uniform_(self.U.weight); nn.init.xavier_uniform_(self.I.weight)
        self.w_s = nn.Parameter(torch.tensor(0.5))  # learnable sentiment weight
    def score(self, users, items, senti_dict):
        dot = (self.U(users) * self.I(items)).sum(dim=1)
        pairs = [(int(u.item()), int(i.item())) for u,i in zip(users, items)]
        s = torch.tensor([senti_dict.get(p, 0.0) for p in pairs],
                         dtype=torch.float32, device=users.device)
        return dot + self.w_s * s

def sample_bpr_batch(u_pos_dict, n_items, batch_size=4096):
    keys=[u for u,v in u_pos_dict.items() if len(v)>0]
    users,pos,neg=[],[],[]
    for _ in range(batch_size):
        u=random.choice(keys)
        ip=random.choice(tuple(u_pos_dict[u]))
        # sample negative not in pos set
        while True:
            j=random.randint(0,n_items-1)
            if j not in u_pos_dict[u]: break
        users.append(u); pos.append(ip); neg.append(j)
    return (torch.tensor(users,device=device),
            torch.tensor(pos,device=device),
            torch.tensor(neg,device=device))

def bpr_loss(pos_scores, neg_scores):
    return -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-9))

# sentiment lookup dicts (use original sentiment on val/test, and on kept train)
def make_senti_lookup(df_part):
    d={}
    for r in df_part.itertuples(index=False):
        d[(int(r.u), int(r.i))] = float(r.sentiment_score)
    return d
senti_train = make_senti_lookup(train_clean)  # cleaned
senti_val   = make_senti_lookup(val_df)
senti_test  = make_senti_lookup(test_df)

model = BPRMF_Sent(n_users, n_items, dim=64).to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)

# ---------------------------
# 3) Evaluation helpers
# ---------------------------
@torch.no_grad()
def get_scores_user_vs_items(model, u, cand_items, senti_dict):
    u_t = torch.tensor([u]*len(cand_items), device=device)
    i_t = torch.tensor(cand_items, device=device)
    return model.score(u_t, i_t, senti_dict).detach().cpu().numpy()

@torch.no_grad()
def evaluate(model, train_pos, eval_pos, senti_dict, K_list=(10,20), sample_neg=1000):
    metrics = {f"{m}@{k}":[] for k in K_list for m in ["Recall","NDCG","MRR","Hit"]}
    users = sorted(eval_pos.keys()); all_items = np.arange(n_items)
    for u in users:
        pos_items = list(eval_pos[u])
        if not pos_items: continue
        seen = train_pos.get(u, set())
        neg_pool = np.setdiff1d(all_items, np.array(list(seen)))
        neg_cands = np.random.choice(neg_pool, size=min(sample_neg, len(neg_pool)), replace=False) if len(neg_pool)>0 else np.array([], int)
        candidates = np.unique(np.concatenate([neg_cands, np.array(pos_items)]))
        scores = get_scores_user_vs_items(model, u, candidates, senti_dict)
        order = np.argsort(-scores); ranked_items = candidates[order]
        for K in K_list:
            topk = ranked_items[:K]
            hits = [1 if it in pos_items else 0 for it in topk]
            recall = sum(hits)/len(pos_items)
            dcg = sum(1.0/math.log2(idx+2) for idx,it in enumerate(topk) if it in pos_items)
            idcg = sum(1.0/math.log2(i+2) for i in range(min(len(pos_items),K)))
            ndcg = dcg/idcg if idcg>0 else 0.0
            rr = 0.0
            for idx,it in enumerate(ranked_items):
                if it in pos_items: rr = 1.0/(idx+1); break
            metrics[f"Recall@{K}"].append(recall)
            metrics[f"NDCG@{K}"].append(ndcg)
            metrics[f"MRR@{K}"].append(rr)
            metrics[f"Hit@{K}"].append(1 if any(hits) else 0)
    return {m: float(np.mean(v)) if v else 0.0 for m,v in metrics.items()}

@torch.no_grad()
def auc_score(model, train_pos, eval_pos, senti_dict, neg_per_pos=50):
    aucs=[]
    for u,pos_set in eval_pos.items():
        pos_items=list(pos_set)
        if not pos_items: continue
        seen=train_pos.get(u,set())
        all_negs=[j for j in range(n_items) if j not in seen]
        if len(all_negs)==0: continue
        negs=random.sample(all_negs,min(neg_per_pos,len(all_negs)))
        sp = get_scores_user_vs_items(model, u, pos_items, senti_dict)
        sn = get_scores_user_vs_items(model, u, negs, senti_dict)
        wins=sum(p>n for p in sp for n in sn)
        total=len(sp)*len(sn)
        if total>0: aucs.append(wins/total)
    return float(np.mean(aucs)) if aucs else 0.0

@torch.no_grad()
def accuracy_score(model, eval_df, senti_dict, threshold=0.5):
    preds=[]
    for r in eval_df.itertuples(index=False):
        s = model.score(torch.tensor([int(r.u)],device=device),
                        torch.tensor([int(r.i)],device=device),
                        senti_dict)[0].item()
        preds.append(s)
    preds=np.array(preds)
    preds_norm=(preds - preds.min())/(preds.max()-preds.min()+1e-8)
    if "rating" in eval_df.columns:
        labels=(pd.to_numeric(eval_df["rating"],errors="coerce").fillna(0).values>0).astype(int)
    else:
        labels=np.ones_like(preds_norm)
    return float(((preds_norm>=threshold).astype(int)==labels).mean())

@torch.no_grad()
def rmse_score(model, eval_df, senti_dict):
    preds=[]
    for r in eval_df.itertuples(index=False):
        s = model.score(torch.tensor([int(r.u)],device=device),
                        torch.tensor([int(r.i)],device=device),
                        senti_dict)[0].item()
        preds.append(s)
    preds=np.array(preds)
    if "rating" in eval_df.columns:
        labels=pd.to_numeric(eval_df["rating"],errors="coerce").fillna(0).values.astype(float)
    else:
        labels=np.ones_like(preds)
    preds_scaled=(preds-preds.min())/(preds.max()-preds.min()+1e-8)*(labels.max()-labels.min())+labels.min()
    return float(np.sqrt(np.mean((preds_scaled-labels)**2)))

# ---------------------------
# 4) Train backbone on S_clean
# ---------------------------
EPOCHS=50
BATCH_SIZE=4096
eval_every=5
print("Training sentiment-aware BPRMF on S_clean...")
for epoch in range(1, EPOCHS+1):
    model.train()
    users,pos_items,neg_items = sample_bpr_batch(u_train_pos_clean, n_items, BATCH_SIZE)
    pos_scores = model.score(users, pos_items, senti_train)
    neg_scores = model.score(users, neg_items, senti_train)
    loss = bpr_loss(pos_scores, neg_scores)
    opt.zero_grad(); loss.backward(); opt.step()

    if epoch % eval_every == 0 or epoch == 1:
        model.eval()
        val_metrics = evaluate(model, u_train_pos_clean, u_val_pos, senti_val, K_list=(10,20))
        val_auc  = auc_score(model, u_train_pos_clean, u_val_pos, senti_val)
        val_acc  = accuracy_score(model, val_df, senti_val)
        val_rmse = rmse_score(model, val_df, senti_val)
        print(f"[Epoch {epoch:03d}] Loss={loss.item():.4f} | "
              f"Val R@10={val_metrics['Recall@10']:.4f} N@10={val_metrics['NDCG@10']:.4f} "
              f"MRR@10={val_metrics['MRR@10']:.4f} Hit@10={val_metrics['Hit@10']:.4f} | "
              f"AUC={val_auc:.4f} Acc={val_acc:.4f} RMSE={val_rmse:.4f} | w_s={model.w_s.item():.3f}")

# ---------------------------
# 5) Final TEST table
# ---------------------------
print("\nFinal evaluation on TEST — SVV + ABSA-BERT (Ours):")
model.eval()
test_metrics = evaluate(model, u_train_pos_clean, u_test_pos, senti_test, K_list=(10,20))
test_auc  = auc_score(model, u_train_pos_clean, u_test_pos, senti_test)
test_acc  = accuracy_score(model, test_df, senti_test)
test_rmse = rmse_score(model, test_df, senti_test)

results = {
    "Method": "SVV + ABSA-BERT (Ours)",
    "Recall@10": test_metrics['Recall@10'],
    "Recall@20": test_metrics['Recall@20'],
    "NDCG@10":   test_metrics['NDCG@10'],
    "NDCG@20":   test_metrics['NDCG@20'],
    "MRR@10":    test_metrics['MRR@10'],
    "MRR@20":    test_metrics['MRR@20'],
    "Accuracy":  test_acc,
    "AUC":       test_auc,
    "RMSE":      test_rmse
}
print(pd.DataFrame([results]).round(4).to_string(index=False))